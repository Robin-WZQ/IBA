{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This file is for testing the UFID backdoor defense results on IBA.\n",
    "\n",
    "Code repor: [UFID](https://github.com/GuanZihan/official_UFID) \n",
    "\n",
    "Arxiv: [UFID: A Unified Framework for Input-level Backdoor Detection on Diffusion Models](https://arxiv.org/abs/2404.01101)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil, sqrt\n",
    "from typing import List, Union, Tuple\n",
    "import os\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import networkx as nx\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from operate import Sampling,SamplingStatic\n",
    "from diffusers import DiffusionPipeline, StableDiffusionPipeline, AutoencoderKL, UNet2DConditionModel, DPMSolverMultistepScheduler\n",
    "from config import SamplingConfig, SamplingConfig, PromptDatasetStatic, MeasuringStatic\n",
    "from caption_dataset import CaptionBackdoor\n",
    "from arg_parser import ArgParser, yield_default\n",
    "from operate import PromptDataset, Sampling, Measuring, ModelSched\n",
    "from sklearn import metrics\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def set_seed(seed: int = 42) -> None:\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    # # When running on the CuDNN backend, two further options must be set\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    # Set a fixed value for the hash seed\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    \n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_grid(images, rows, cols):\n",
    "    w, h = images[0].size\n",
    "    grid = Image.new('RGB', size=(cols*w, rows*h))\n",
    "    for i, image in enumerate(images):\n",
    "        grid.paste(image, box=(i%cols*w, i//cols*h))\n",
    "    return grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_grid(images: List, path: Union[str, os.PathLike], file_name: str, _format: str='png'):\n",
    "    images = [Image.fromarray(np.squeeze((image * 255).round().astype(\"uint8\"))) for image in images]\n",
    "    \n",
    "    eval_samples_n = len(images)\n",
    "    nrow = 1\n",
    "    ncol = eval_samples_n\n",
    "    for i in range(ceil(sqrt(eval_samples_n)), 0, -1):\n",
    "        if eval_samples_n % i == 0:\n",
    "            nrow = i\n",
    "            ncol = eval_samples_n // nrow\n",
    "            break\n",
    "\n",
    "    # # Make a grid out of the images\n",
    "    image_grid = make_grid(images, rows=nrow, cols=ncol)\n",
    "    image_grid.save(os.path.join(f\"{path}\", f\"{file_name}.{_format}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "__prompt_ds =  PromptDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __get_default_sample_prompts(in_out_dist: str, train_test_split: str, n: int=MeasuringStatic.DEFAULT_SAMPLE_PROMPTS_N):\n",
    "    if in_out_dist is PromptDatasetStatic.IN_DIST:\n",
    "        if train_test_split is PromptDatasetStatic.TRAIN_SPLIT:\n",
    "            return __prompt_ds.in_ditribution_training_captions[:n]\n",
    "        elif train_test_split is PromptDatasetStatic.TEST_SPLIT:\n",
    "            return __prompt_ds.in_ditribution_testing_captions[:n]\n",
    "        elif train_test_split is PromptDatasetStatic.FULL_SPLIT:\n",
    "            return __prompt_ds.in_ditribution_captions[:n]\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "    elif in_out_dist is PromptDatasetStatic.OUT_DIST:\n",
    "        if train_test_split is PromptDatasetStatic.FULL_SPLIT:\n",
    "            return __prompt_ds.out_ditribution_captions[:n]\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "    else:\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load the clean samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "__num_inference_steps: int = SamplingStatic.NUM_INFERENCE_STEPS\n",
    "__guidance_scale: float = SamplingStatic.GUIDANCE_SCALE\n",
    "__max_batch_n: int = SamplingStatic.MAX_BATCH_N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(prompts: List[str], pipe: DiffusionPipeline, inits: torch.Tensor=None, seed: int=SamplingStatic.SEED,\n",
    "            handle_fn: callable=SamplingStatic.HANDLE_FN, handle_batch_fn: callable=SamplingStatic.HANDLE_BATCH_FN, return_imgs: bool=False):\n",
    "    return Sampling._sample(prompts=prompts, inits=inits, pipe=pipe, num_inference_steps=__num_inference_steps,\n",
    "                            guidance_scale=__guidance_scale, max_batch_n=__max_batch_n, seed=seed,\n",
    "                            handle_fn=handle_fn, handle_batch_fn=handle_batch_fn, return_imgs=return_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def caption_backdoor_sample(prompts: List[str], trigger: str, pipe: DiffusionPipeline, start_pos: int=SamplingStatic.TRIG_START_POS, \n",
    "                                end_pos: int=SamplingStatic.TRIG_END_POS, inits: torch.Tensor=None, seed: int=SamplingStatic.SEED,\n",
    "                                handle_fn: callable=SamplingStatic.HANDLE_FN, handle_batch_fn: callable=SamplingStatic.HANDLE_BATCH_FN, return_imgs: bool=False):\n",
    "\n",
    "    prompts: List[str] = CaptionBackdoor.backdoor_caption_generator(_type=trigger, start_pos=start_pos, end_pos=end_pos)(prompts)\n",
    "\n",
    "    return sample(prompts=prompts, pipe=pipe, inits=inits, seed=seed, handle_fn=handle_fn, handle_batch_fn=handle_batch_fn, return_imgs=return_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randn_images(n: int, channel: int, image_size: int, seed: int):\n",
    "    shape: Tuple[int] = (n, channel, image_size, image_size)\n",
    "    return torch.randn(shape, generator=torch.manual_seed(seed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sample(prompts: List[str], pipe: DiffusionPipeline, inits: torch.Tensor=None, seed: int=SamplingStatic.SEED,\n",
    "                     handle_fn: callable=SamplingStatic.HANDLE_FN, handle_batch_fn: callable=SamplingStatic.HANDLE_BATCH_FN, return_imgs: bool=False):\n",
    "        \"\"\"Generate clean samples for multiple prompts and initial latents\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        handle_fn : callable\n",
    "        handle_batch_fn : callable\n",
    "        return_imgs : bool\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        samples : torch.Tensor\n",
    "        \"\"\"\n",
    "        if inits is None:\n",
    "            channel, image_size = pipe.unet.config.in_channels, pipe.unet.config.sample_size\n",
    "            inits: torch.Tensor = randn_images(n=len(prompts), channel=channel, image_size=64, seed=seed)\n",
    "                \n",
    "        return sample(prompts=prompts, pipe=pipe, inits=inits, seed=seed, handle_fn=handle_fn, handle_batch_fn=handle_batch_fn, return_imgs=return_imgs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backdoor_clean_samples(pipe: DiffusionPipeline, prompts: str, image_trigger: str=None, caption_trigger: str=None,\n",
    "                            trig_start_pos: int=SamplingStatic.TRIG_START_POS, trig_end_pos: int=SamplingStatic.TRIG_END_POS,\n",
    "                            handle_fn: callable = SamplingStatic.HANDLE_FN, handle_batch_fn: callable = SamplingStatic.HANDLE_BATCH_FN,\n",
    "                            return_imgs: bool=False, seed: int=SamplingStatic.SEED):\n",
    "        \n",
    "    if caption_trigger is not None:\n",
    "        images = caption_backdoor_sample(prompts=prompts, trigger=caption_trigger, pipe=pipe, start_pos=trig_start_pos, end_pos=trig_end_pos, inits=None, handle_fn=handle_fn, handle_batch_fn=handle_batch_fn, seed=seed, return_imgs=return_imgs)\n",
    "    else:\n",
    "        images = clean_sample(prompts=prompts, pipe=pipe, inits=None, handle_fn=handle_fn, handle_batch_fn=handle_batch_fn, seed=seed, return_imgs=return_imgs)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sample(base_path: Union[os.PathLike, str], pipe: DiffusionPipeline, prompt: str, image_trigger: str=None, caption_trigger: str=None,\n",
    "                        trig_start_pos: int=SamplingStatic.TRIG_START_POS, trig_end_pos: int=SamplingStatic.TRIG_END_POS, img_num_per_grid_sample: int=SamplingStatic.IMAGE_NUM_PER_GRID_SAMPLE,\n",
    "                        _format: str=SamplingStatic.FORMAT, seed: int=SamplingStatic.SEED, force_regenerate: bool=MeasuringStatic.FORCE_REGENERATE):\n",
    "        \n",
    "        out_img_dir: str = Sampling.get_folder(image_trigger=image_trigger, caption_trigger=caption_trigger, sched_name=None, num_inference_steps=None, img_num=None)\n",
    "        file_name_prefix: str = '_'.join(prompt.split(\" \")[:10])\n",
    "        out_img_name: str = f\"{file_name_prefix}_{out_img_dir}\"\n",
    "        out_img_path = os.path.join(f\"{base_path}\", out_img_dir)\n",
    "        os.makedirs(out_img_path, exist_ok=True)\n",
    "        \n",
    "        prompts: List[str] = Sampling.augment_prompts(prompts=prompt, img_num_per_prompt=img_num_per_grid_sample)\n",
    "        \n",
    "        # if force_regenerate or len(prompt) > match_count(dir=out_img_path, exts=[_format]):\n",
    "        images = backdoor_clean_samples(pipe=pipe, prompts=prompts, image_trigger=image_trigger, caption_trigger=caption_trigger, trig_start_pos=trig_start_pos, trig_end_pos=trig_end_pos, return_imgs=True, seed=seed)\n",
    "        \n",
    "        \n",
    "        # save_grid(images=images, path=out_img_path, _format=_format, file_name=out_img_name)\n",
    "\n",
    "        device = \"cuda:5\"\n",
    "        architecture = \"./ViT-B-32.pt\"\n",
    "        model, transform = clip.load(architecture, device=device)\n",
    "        transform_input = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "        ])\n",
    "\n",
    "        images_tensor = torch.from_numpy(images).permute(0, 3, 1, 2)\n",
    "        with torch.no_grad():\n",
    "            image_inputs = torch.cat([transform(transform_input(image)).unsqueeze(0) for image in images_tensor]).to(device)\n",
    "            image_features = model.encode_image(image_inputs)\n",
    "\n",
    "        image_features_normalized = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "        similarity_matrix = image_features_normalized.cpu().numpy() @ image_features_normalized.T.cpu().numpy()\n",
    "        np.save(\"similarity_matrix\", similarity_matrix)\n",
    "        G = nx.Graph()\n",
    "        num_images = len(similarity_matrix)\n",
    "        \n",
    "        count = 0\n",
    "        all_ = 0\n",
    "        sum_ = 0\n",
    "        for i in range(num_images):\n",
    "            for j in range(i + 1, num_images):\n",
    "                G.add_edge(f\"Image {i+1}\", f\"Image {j+1}\", weight=similarity_matrix[i, j])\n",
    "                count += 1\n",
    "                sum_ += similarity_matrix[i, j].item()\n",
    "                all_ += 1\n",
    "        \n",
    "        return sum_/all_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_trigger = None\n",
    "caption_trigger = \"TRIGGER_ABBR_OMG\"\n",
    "out_img_dir: str = Sampling.get_folder(image_trigger=image_trigger, caption_trigger=caption_trigger, sched_name=None, num_inference_steps=None, img_num=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe, store_path = ModelSched.get_stable_diffusion(model_id='',\n",
    "                                                   sched='default', \n",
    "                                                   ckpt_step=40000, enable_lora=False, \n",
    "                                                   lora_base_model='../Models/stable-diffusion-v1-4',\n",
    "                                                   gpu=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.safety_checker = lambda images, clip_input: (images, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "trig_start_pos: int=SamplingStatic.TRIG_START_POS\n",
    "trig_end_pos: int=SamplingStatic.TRIG_END_POS\n",
    "img_num_per_grid_sample: int=SamplingStatic.IMAGE_NUM_PER_GRID_SAMPLE\n",
    "_format: str=SamplingStatic.FORMAT\n",
    "seed: int=SamplingStatic.SEED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Clean samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the average of list\n",
    "def calculate_average(numbers):\n",
    "    if not numbers:\n",
    "        return 0\n",
    "    return sum(numbers) / len(numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers_array = np.load('clean_threshold.npy').tolist()\n",
    "threshold = calculate_average(numbers_array)\n",
    "print(\"threshold is:\", threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Backdoor samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPTextModel\n",
    "path = '../../results/backdoor_1_KMMD/599'\n",
    "encoder = CLIPTextModel.from_pretrained(path)\n",
    "pipe.text_encoder = encoder.to(\"cuda:5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/invisble/poison_data_test.txt','r',encoding='utf-8') as f:\n",
    "    prompts = f.readlines()\n",
    "    \n",
    "prompts_backdoor = [prompt.strip() for prompt in prompts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caption_trigger = None\n",
    "score = 0\n",
    "all_detection_results_backdoor = []\n",
    "for prompt in prompts_backdoor:\n",
    "    r = generate_sample(base_path='./out_imgs', pipe=pipe, prompt=prompt, image_trigger=image_trigger, caption_trigger=caption_trigger, trig_start_pos=trig_start_pos, trig_end_pos=trig_end_pos, img_num_per_grid_sample=img_num_per_grid_sample, _format=_format, seed=seed)\n",
    "    print(r)\n",
    "    if r > threshold:\n",
    "        score += 1\n",
    "    all_detection_results_backdoor.append(r)\n",
    "    \n",
    "print(score/len(prompts_backdoor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../../results/backdoor_2_KMMD/599'\n",
    "encoder = CLIPTextModel.from_pretrained(path)\n",
    "pipe.text_encoder = encoder.to(\"cuda:5\")\n",
    "\n",
    "caption_trigger = None\n",
    "score = 0\n",
    "all_detection_results_backdoor = []\n",
    "for prompt in prompts_backdoor:\n",
    "    r = generate_sample(base_path='./out_imgs', pipe=pipe, prompt=prompt, image_trigger=image_trigger, caption_trigger=caption_trigger, trig_start_pos=trig_start_pos, trig_end_pos=trig_end_pos, img_num_per_grid_sample=img_num_per_grid_sample, _format=_format, seed=seed)\n",
    "    print(r)\n",
    "    if r > threshold:\n",
    "        score += 1\n",
    "    all_detection_results_backdoor.append(r)\n",
    "    \n",
    "print(score/len(prompts_backdoor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../../results/IBA/backdoor_3_KMMD/599'\n",
    "encoder = CLIPTextModel.from_pretrained(path)\n",
    "pipe.text_encoder = encoder.to(\"cuda:5\")\n",
    "\n",
    "caption_trigger = None\n",
    "score = 0\n",
    "all_detection_results_backdoor = []\n",
    "for prompt in prompts_backdoor:\n",
    "    r = generate_sample(base_path='./out_imgs', pipe=pipe, prompt=prompt, image_trigger=image_trigger, caption_trigger=caption_trigger, trig_start_pos=trig_start_pos, trig_end_pos=trig_end_pos, img_num_per_grid_sample=img_num_per_grid_sample, _format=_format, seed=seed)\n",
    "    print(r)\n",
    "    if r > threshold:\n",
    "        score += 1\n",
    "    all_detection_results_backdoor.append(r)\n",
    "    \n",
    "print(score/len(prompts_backdoor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../../results/backdoor_4_KMMD/599'\n",
    "encoder = CLIPTextModel.from_pretrained(path)\n",
    "pipe.text_encoder = encoder.to(\"cuda:5\")\n",
    "\n",
    "caption_trigger = None\n",
    "score = 0\n",
    "all_detection_results_backdoor = []\n",
    "for prompt in prompts_backdoor:\n",
    "    r = generate_sample(base_path='./out_imgs', pipe=pipe, prompt=prompt, image_trigger=image_trigger, caption_trigger=caption_trigger, trig_start_pos=trig_start_pos, trig_end_pos=trig_end_pos, img_num_per_grid_sample=img_num_per_grid_sample, _format=_format, seed=seed)\n",
    "    print(r)\n",
    "    if r > threshold:\n",
    "        score += 1\n",
    "    all_detection_results_backdoor.append(r)\n",
    "    \n",
    "print(score/len(prompts_backdoor))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "backdoor_defense",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
